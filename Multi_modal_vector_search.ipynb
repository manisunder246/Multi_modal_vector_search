{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c9aae8-9c34-42c9-9167-c3509dc2a574",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ddc398-1eee-4149-b43d-9001815d3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LayoutLMv2Processor, AutoModelForTokenClassification, LayoutLMv2ForTokenClassification, AutoTokenizer\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from io import BytesIO\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "import glob\n",
    "import os\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import nltk\n",
    "import openai\n",
    "import uuid\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f35487d-b08e-4186-9fbe-f19105fff585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Ensure PyTorch uses GPU if available on MacBook with MPS or fallback to CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976581ab-fc98-4447-ae01-cc872433c7bf",
   "metadata": {},
   "source": [
    "## 2.Setting Paths and Extracting PDF Content with Detailed Configuration\n",
    "\n",
    "- This code sets up paths for NLTK data and image extraction, then processes a PDF using partition_pdf.\n",
    "- It extracts text and images from the PDF with detailed settings, saving images to a specified directory and handling text chunking for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f202ac8-342c-4f7b-86fc-c57c004e5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"your_path/Multi_modal_vector_search/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fe7738-3e50-48ac-8d77-f3a286ee4a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Set the NLTK data path\n",
    "nltk_data_path = \"../nltk_data/nltk_data\"\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Define the path to the directory containing extracted images\n",
    "IMG_DIR = \"your_path/Multi_modal_vector_search/images/\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# Re-run partition_pdf with detailed settings\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path + \"LLaMA2.pdf\",\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=IMG_DIR,\n",
    "    extract_image_block_types=[\"Image\"],\n",
    "    extract_image_block_to_payload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae734d1e-3709-4463-95d0-f3ea5cea6cea",
   "metadata": {},
   "source": [
    "## 2. Examining the PDF elements\n",
    "- Categorizing PDF Elements and Counting Tables and Text:\n",
    "    - This code analyzes the PDF elements by counting their occurrences and categorizing them into \"table\" and \"text\" types using a custom Element class.\n",
    "    - It extracts the number of tables and text elements separately to understand the content structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af505f42-4729-4264-b3c5-0337f9838865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 111,\n",
       " \"<class 'unstructured.documents.elements.Table'>\": 44,\n",
       " \"<class 'unstructured.documents.elements.TableChunk'>\": 6}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cbbcea2-4bd6-4f4e-9653-6ff24c881818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of table elements: 50\n",
      "Number of text elements: 111\n"
     ]
    }
   ],
   "source": [
    "# Define a custom Element class for structured categorization\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "# Categorize PDF elements by type (table or text)\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    # If the element is of type 'Table', categorize as table\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    # If the element is of type 'CompositeElement', categorize as text\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Extract and count table elements\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(\"Number of table elements:\", len(table_elements))\n",
    "\n",
    "# Extract and count text elements\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(\"Number of text elements:\", len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32442f55-739b-4947-a089-8be7a45bbad6",
   "metadata": {},
   "source": [
    "### Quick check to see the table elements too for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6649bf7c-04fe-4df1-b37a-dd2edcaaba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Element(type='table', text='Time Power Carbon Emitted (GPU hours) Consumption (W) (tCOzeq) 7B 184320 400 31.22 L 13B 368640 400 62.44 TAMA 2 348 1038336 350 153.90 70B 1720320 400 291.42 Total 3311616 539.00')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_elements[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffd456-7cda-4580-96aa-99bb5b5287d8",
   "metadata": {},
   "source": [
    "## 3. Multi-vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2488413d-ec92-44c6-9fd0-118217a7c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from function_calling1 import _parse_google_docstring  # Use the local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7ba2e65-e39a-4c3e-98d8-fcc322f5bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73d4de16-6daf-47af-b6f8-b172b376fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "OPENAI_API_KEY = \"you_api_key\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\",api_key=OPENAI_API_KEY, max_tokens=1500 )\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fea812f-e48e-45d0-9bcd-0a3ce2854dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ebc3b07-633e-4c3a-8367-a16957901d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The table summarizes the power consumption and carbon emissions for different models based on GPU hours. The models listed are 7B, 13B, TAMA 2, and 70B. The power consumption for each model is 400W, except for TAMA 2, which consumes 350W. The carbon emissions (in tCO2eq) for each model are as follows: 7B emits 31.22 tCO2eq, 13B emits 62.44 tCO2eq, TAMA 2 emits 153.90 tCO2eq, and 70B emits 291.42 tCO2eq. The total power consumption across all models is 3,311,616 GPU hours, resulting in a total carbon emission of 539.00 tCO2eq.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaa245b6-99fa-4d1f-9382-4e5a58f20786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the concatenated text\n",
    "output_text = \"\"\n",
    "for i, summary in enumerate(table_summaries, 1):\n",
    "    output_text += f\"Table {i} details:\\n{summary}\\n\\n\\n\"\n",
    "\n",
    "# Save the concatenated text to a file\n",
    "with open('table_summaries.txt', 'w') as file:\n",
    "    file.write(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecc63150-17aa-4097-a5c8-3b97b5512c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the concatenated text\n",
    "output_text = \"\"\n",
    "for i, summary in enumerate(table_summaries, 1):\n",
    "    output_text += f\"Table {i} details:\\n{summary}\\n\\n\\n\"\n",
    "\n",
    "# Save the concatenated text to a file\n",
    "with open('table_summaries.txt', 'w') as file:\n",
    "    file.write(output_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cffb0108-ea1c-4510-a05f-a87b4cddedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dfadff4-e184-4ab1-b634-3fae45389498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models (LLMs) have demonstrated significant potential as AI assistants, excelling in complex reasoning tasks across various domains. They interact with users through chat interfaces, leading to widespread adoption. LLMs are trained using auto-regressive transformers on large datasets, followed by alignment with human preferences through techniques like Reinforcement Learning with Human Feedback (RLHF). However, the high computational demands restrict their development to a few entities.\\n\\nPublicly released LLMs like BLOOM, LLaMa-1, and Falcon match the performance of closed models like GPT-3 and Chinchilla but are not as fine-tuned for usability and safety as closed models such as ChatGPT, BARD, and Claude. Fine-tuning these models involves significant computational and human annotation costs, limiting transparency and reproducibility.\\n\\nThis work introduces Llama 2, a family of pretrained and fine-tuned LLMs, including Llama 2 and Llama 2-Cuar, with up to 70 billion parameters. These models perform well on helpfulness and safety benchmarks, comparable to some closed-source models. The paper details the fine-tuning methodology and safety improvements, aiming to enable the community to reproduce and enhance LLM safety. Observations during development include tool usage and temporal knowledge organization.\\n\\nFigure 3 shows safety evaluation results for Llama 2-Cuar against other models, highlighting inherent biases in LLM evaluations. The models are released for public research and commercial use.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638f4cd-ad9a-43fd-bf28-d51a3d6dd2c7",
   "metadata": {},
   "source": [
    "## 3.2. Extracting images and summarizing them\n",
    "\n",
    "- This section extracts images from PDF files and uses pytesseract for Optical Character Recognition (OCR) to obtain text from the images.\n",
    "- The extracted text is then combined with image details to create a summary using GPT-4, which generates a detailed 5-point summary based on the content and context of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c902ac0-0a95-4cd5-8279-7cd7941ab8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the output directory\n",
    "import fitz \n",
    "# Define the path to the directory containing extracted images\n",
    "IMG_DIR = \"your_path/Multi_modal_vector_search/images/\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91de5e15-55a6-4f4e-bb43-b81fa6687058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the PDF\n",
    "pdf_document = fitz.open(\"your_path/Multi_modal_vector_search/LLaMA2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4d3bbb5-d4c7-4f1d-a673-69ddd0260ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# Function to save image as PNG\n",
    "def save_image_as_png(image_bytes, page_number, image_index):\n",
    "    image_filename = f\"image_{page_number}_{image_index}.png\"\n",
    "    image_path = os.path.join(IMG_DIR, image_filename)\n",
    "    with open(image_path, \"wb\") as image_file:\n",
    "        image_file.write(image_bytes)\n",
    "\n",
    "# Extract images\n",
    "for page_number in range(len(pdf_document)):\n",
    "    page = pdf_document[page_number]\n",
    "    image_list = page.get_images(full=True)\n",
    "    xrefs = [img[0] for img in image_list]\n",
    "    \n",
    "    for image_index, xref in enumerate(xrefs):\n",
    "        base_image = pdf_document.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        save_image_as_png(image_bytes, page_number, image_index)\n",
    "\n",
    "    # Additional extraction for figures that are not direct images\n",
    "    for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "        if \"lines\" in block:  # Check if the block contains text lines\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    if \"font\" in span and \"image\" in span[\"font\"]:  # Check if span contains an image\n",
    "                        image_bytes = span[\"image\"]\n",
    "                        save_image_as_png(image_bytes, page_number, image_index)\n",
    "                        image_index += 1\n",
    "\n",
    "print(\"Image extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972a4de-1cf0-4c04-bf79-893b49268449",
   "metadata": {},
   "source": [
    "## 3.3 Text summary of each image\n",
    "\n",
    "- For each extracted image, the text obtained through OCR is summarized using GPT-4.\n",
    "- The summary provides insights into the type of image (e.g., graph, table), relevant labels, and key metrics, enabling a more comprehensive understanding of the visual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6077f3c0-725a-4762-bc97-98b2b8970a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize and compress image\n",
    "def resize_and_compress_image(image_path, max_size=(500, 500)):\n",
    "    with Image.open(image_path) as img:\n",
    "        img.thumbnail(max_size)\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return buffer.getvalue()\n",
    "\n",
    "def generate_image_summary(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    resized_image_bytes = resize_and_compress_image(image_path)\n",
    "    encoded_image = base64.b64encode(resized_image_bytes).decode('utf-8')\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are an assistant tasked with summarizing images. \"\n",
    "        \"The image you're provided with is from a research paper. \"\n",
    "        \"It can have graphs of various kinds, like heat-maps, histograms, scatter plots etc. \"\n",
    "        \"Identify the type of graph, go through the X and Y coordinates and the labels and try to understand what is being plotted. \"\n",
    "        \"From the metrics in the graph (i.e., the numerical values), try to identify some max and min values. \"\n",
    "        \"Use the above data AND your understanding to present a 5-point summary IN DETAIL.\\n\\n\"\n",
    "        f\"Extracted Text: {ocr_text}\"\n",
    "        f\"Image: {encoded_image}\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant tasked with summarizing images. Provide a detailed description of the image.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa4b31-6906-414b-a9f6-67be6e7fa736",
   "metadata": {},
   "source": [
    "## 3.4 Generating and Storing Image Summaries\n",
    "\n",
    "- This code generates summaries for all extracted images by calling the OpenAI GPT-4 API, saving each summary in a corresponding text file.\n",
    "- It reads back the summaries, stores them in a list, and cleans up any residual logging information. This step ensures accurate and organized summaries, ready for further analysis or display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec17bf-63f0-4f67-b598-be5c444534df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_API_TOKEN = \"your-api-key\"\n",
    "client = OpenAI(api_key=OPENAI_API_TOKEN)\n",
    "# Generate summaries for all images and store in respective text files\n",
    "for img_path in os.listdir(IMG_DIR):\n",
    "    if img_path.endswith(\".png\"):\n",
    "        full_img_path = os.path.join(IMG_DIR, img_path)\n",
    "        summary = generate_image_summary(full_img_path)\n",
    "        \n",
    "        # Store the summary in a text file\n",
    "        base_name = os.path.splitext(img_path)[0]\n",
    "        summary_path = os.path.join(IMG_DIR, f\"{base_name}_summary.txt\")\n",
    "        with open(summary_path, \"w\") as summary_file:\n",
    "            summary_file.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed9a7263-8f05-45fd-bc17-c820dc9d763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image summaries generated and cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Read the summaries back from the text files\n",
    "file_paths = glob.glob(os.path.expanduser(os.path.join(IMG_DIR, \"*_summary.txt\")))\n",
    "# Store the content of each summary file in a list\n",
    "img_summaries = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        img_summaries.append(file.read())\n",
    "\n",
    "# Clean up residual logging if needed\n",
    "cleaned_img_summary = [\n",
    "    s.split(\"clip_model_load: total allocated memory: 201.27 MB\\n\\n\", 1)[1].strip()\n",
    "    if \"clip_model_load: total allocated memory\" in s else s\n",
    "    for s in img_summaries\n",
    "]\n",
    "\n",
    "print(\"Image summaries generated and cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71b20b9e-32de-4515-86a5-74ce02ae2e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Detailed Summary of the Image:\\n\\n1. **Type of Graph**: The image appears to be a complex visual representation from a research paper, likely containing multiple types of graphs such as scatter plots, histograms, and heat maps. The presence of various data points and color gradients suggests a detailed analysis of a specific dataset.\\n\\n2. **X and Y Coordinates and Labels**: The X and Y coordinates are not explicitly mentioned in the extracted text, but typical research graphs would have these axes labeled with relevant metrics such as time, frequency, or other measurable variables. The labels would provide context to the data being analyzed, such as \"Time (s)\" on the X-axis and \"Frequency (Hz)\" on the Y-axis for a scatter plot.\\n\\n3. **Understanding the Plotted Data**: The data plotted likely represents a detailed analysis of a specific phenomenon or experiment. For instance, a heat map might show the intensity of a particular variable across different conditions, while a scatter plot could illustrate the relationship between two variables. The extracted text mentions historical data and a query about the 2nd World War, suggesting a historical or temporal analysis.\\n\\n4. **Max and Min Values**: The numerical values in the graph would indicate the range of the data. For example, in a heat map, the color gradient would show the intensity from minimum to maximum values. In a histogram, the height of the bars would represent the frequency of data points within specific ranges. Identifying these max and min values would be crucial for interpreting the data\\'s spread and central tendencies.\\n\\n5. **Context and Interpretation**: The extracted text mentions a knowledge cutoff date of 1940 and a query about the 2nd World War, indicating that the data might be historical. The graphs could be analyzing trends or patterns over time, possibly related to historical events or phenomena. The detailed metrics and visual representations would help in understanding the underlying trends and drawing conclusions based on the data.\\n\\n### Additional Insights:\\n\\n- **Historical Analysis**: Given the mention of the 2nd World War and the knowledge cutoff date, the graphs might be part of a historical analysis, possibly examining trends leading up to or during the early years of the war.\\n- **Data Visualization**: The use of multiple graph types suggests a comprehensive approach to data visualization, allowing for a multi-faceted analysis of the dataset.\\n- **Research Context**: The detailed nature of the graphs indicates a research context, likely aimed at providing insights or supporting hypotheses within'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_img_summary[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551ea7c-a8f5-408f-a043-881a21cfeadd",
   "metadata": {},
   "source": [
    "## 4. Adding to vectorstore\n",
    "- This section focuses on generating embeddings for different types of content (text, tables, and images) and storing them in a FAISS vector store.\n",
    "- This enables efficient retrieval of multi-modal data based on similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3592cdd-b987-4782-86dd-c44b3c4472f9",
   "metadata": {},
   "source": [
    "### 4.1 Initializing OpenAI Embeddings and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b69d7e36-fe67-405e-95fc-2a1b04ccb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY)\n",
    "# Initialize FAISS index\n",
    "dimension = 1536  # Dimension of the embeddings from text-embedding-ada-002\n",
    "index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b18ac7-d121-455e-a49d-1ec6cdc2efac",
   "metadata": {},
   "source": [
    "#### `Description:`\n",
    "The embeddings are initialized using the text-embedding-ada-002 model from OpenAI, setting the dimension to 1536. A FAISS index is also created to store these embeddings, enabling efficient similarity search for multi-modal data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd4a7d-644a-4de3-876b-8e1e7ceb7f0e",
   "metadata": {},
   "source": [
    "### 4.2 Creating the Document Store and FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d54e49b5-26a8-42fa-b86d-bb806e1d68b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Create the docstore\n",
    "docstore = InMemoryDocstore({})\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f129c-1639-4d6d-9638-c27863481468",
   "metadata": {},
   "source": [
    "#### `Description:` \n",
    "An in-memory document store is created using InMemoryDocstore, while the FAISS vector store is initialized to handle embedding storage, retrieval, and document management. The embeddings function is set to the embed_query function, which converts queries into vector embeddings for similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020cbf0-ea1b-41b2-9bf5-9b6c1c20e44c",
   "metadata": {},
   "source": [
    "### 4.3 Adding Documents, Tables, and Images to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a31644b0-07ac-44fc-b6ce-cffc794d54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to add documents to the vectorstore\n",
    "def add_documents_to_vectorstore(docs, doc_ids):\n",
    "    documents = [\n",
    "        LangchainDocument(page_content=doc, metadata={\"doc_id\": doc_ids[i]})\n",
    "        for i, doc in enumerate(docs)\n",
    "    ]\n",
    "    vectorstore.add_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c016fd9e-2c1b-4452-82be-43be4a262358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for text, tables, and images have been generated and stored in FAISS.\n"
     ]
    }
   ],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "add_documents_to_vectorstore(text_summaries, doc_ids)\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "add_documents_to_vectorstore(table_summaries, table_ids)\n",
    "\n",
    "# Add images\n",
    "img_ids = [str(uuid.uuid4()) for _ in cleaned_img_summary]\n",
    "add_documents_to_vectorstore(cleaned_img_summary, img_ids)\n",
    "\n",
    "print(\"Embeddings for text, tables, and images have been generated and stored in FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a042e-0bd2-4b0f-a262-8e2b3ca7d71b",
   "metadata": {},
   "source": [
    "## 5. Creating the RAG Pipeline\n",
    "\n",
    "- This section sets up a Retrieval-Augmented Generation (RAG) pipeline, which uses retrieved information from the vector store to generate specific answers to user queries using GPT-4.\n",
    "- The pipeline integrates multiple components, including a prompt, retriever, and language model, to handle natural language queries and return accurate response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa94664-d865-47e6-b583-3d122b329c8d",
   "metadata": {},
   "source": [
    "### 5.1 Defining the RAG Prompt and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f52928b3-0f67-4dba-8af7-f546d8f4159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d7aa1b7-ccb2-4911-b29e-3df325d69a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG pipeline\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "Give specific answers.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\",api_key=OPENAI_API_KEY, max_tokens=1500 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68013a97-5f03-48d6-b6ca-b35b0fcadcfc",
   "metadata": {},
   "source": [
    "#### `Description:` \n",
    "A prompt template is created for generating answers based only on the retrieved context, focusing on specificity. The ChatGPT-4 model is initialized with the template to generate responses. The modelâ€™s temperature is set to 0 for deterministic output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd58c4-abc8-4c7c-b000-e8c0214ecc89",
   "metadata": {},
   "source": [
    "### 5.2 Initializing the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9d6c543-12ab-4364-bd0c-8361a3b5f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1cf85-d2d8-4122-9ebf-cef535a732b4",
   "metadata": {},
   "source": [
    "### `Description:` \n",
    "The retriever is set up to fetch the top 5 most similar results from the FAISS vector store using similarity search. This ensures that the most relevant content (text, tables, images) is provided to the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd20b8-6067-482d-886f-9389524cbfe2",
   "metadata": {},
   "source": [
    "### 5.3 Building and Executing the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbfb81-8a45-4884-9618-c7f9424a1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16df003b-c451-4a8e-8f9a-1f65959a8b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The CO2 emissions during pretraining for the Llama-2 models are estimated at 539 tCO2eq.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the value of CO2 emissions during pretraining for Llama-2 7B chat model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64734adf-cc80-4772-82d2-119a94fb0fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The value of GPU hours for pretraining the Llama-2 7B chat model is 3,311,616 GPU hours.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the value of GPU hours for pretraining for Llama-2 7B chat model? Give the specific number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44ca8b86-51ca-406c-9dde-361bac5d6883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The model that performed the best for 'Common sense reasoning' on the overall performance on grouped benchmarks is LAMA2 70B, with a score of 37.5.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Which model performed the best for 'Common sense reasoning' on the Overall performance on grouped benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ef619-17a4-4744-acf3-091d66327829",
   "metadata": {},
   "source": [
    "### Optional - Not tested completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6551f3-1707-4bb0-b616-1534f1c72996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import openai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Set OpenAI API key\n",
    "OPENAI_API_KEY = \"your-api-key\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = 1536  # Dimension of the embeddings from text-embedding-ada-002\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Create the docstore\n",
    "docstore = InMemoryDocstore({})\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# Define the retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# Define the RAG pipeline\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the model (using GPT-4 in this case)\n",
    "model = ChatOpenAI(model=\"gpt-4\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create the RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"QA Bot using GPT-4 and FAISS\")\n",
    "\n",
    "# Input field for the user question\n",
    "question = st.text_input(\"Enter your question:\")\n",
    "\n",
    "if question:\n",
    "    # Ensure question is a string\n",
    "    question = str(question)\n",
    "    \n",
    "    # Perform a search in the FAISS vector store\n",
    "    try:\n",
    "        search_results = retriever.get_relevant_documents(question)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error in retrieving documents: {str(e)}\")\n",
    "    \n",
    "    if search_results:\n",
    "        # Combine the content of the top results to provide context for the LLM\n",
    "        context = \" \".join([result.page_content for result in search_results])\n",
    "        \n",
    "        # Ensure context is a string\n",
    "        context = str(context)\n",
    "        \n",
    "        # Run the RAG pipeline\n",
    "        try:\n",
    "            response = chain.invoke({\"context\": context, \"question\": question})\n",
    "            st.write(\"Answer:\")\n",
    "            st.write(response)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error in RAG pipeline invocation: {str(e)}\")\n",
    "            st.write(f\"Context: {context}\")\n",
    "            st.write(f\"Question: {question}\")\n",
    "    else:\n",
    "        st.error(\"No search results found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
